---
title: "Missing covariate imputation"
execute: 
  freeze: true
knitr:
  opts_chunk: 
    message: false
---

```{r}
#| purl: false
#| include: false
knitr::purl(input = "missing_covariate_imputation.qmd", 
            output = "R/missing_covariate_imputation.R")
```

In this example, we use the `nhanes2` data set from the `mice` R-package to illustrate how to do missing covariate imputation in INLA by using a measurement error model.
As noted in the paper, the `nhanes2` data set is really small, it only has 25 observations and 9 of them are missing, so this is not really a good application of this. However, we chose to use this as it is a data set that is commonly used in other missing data applications in INLA, and so we reasoned that using the same data set would make it easier to compare the implementations.

## Loading packages

```{r}
#| message: false
library(mice)       # Just used for the nhanes2 data set
library(INLA)       # INLA modelling
library(dplyr)      # Data wrangling of the results
library(gt)         # Tables
library(tidyverse)  # Data wrangling and plotting
library(showtext)   # Font
library(colorspace) # Color adjustments
```

```{r}
inla.setOption(num.threads = "1:1")
```

## Loading and preparing the data

```{r}
# Using the nhanes data set found in mice:
data(nhanes2)

nhanes2

n <- nrow(nhanes2)

# Manually dummy-code age:
age2 <- ifelse(nhanes2$age == "40-59", 1, 0)
age3 <- ifelse(nhanes2$age == "60-99", 1, 0)

# Center the response and continuous covariates
chl <- scale(nhanes2$chl, scale = FALSE)[,1]
bmi <- scale(nhanes2$bmi, scale = FALSE)[,1]
```
## Aim
We want to fit the model

$$
chl \sim \beta_0 + \beta_{age2} age_2 + \beta_{age3} age_3 + \beta_{bmi} bmi + \varepsilon,
$$
but there is missingness in bmi, so we will consider two different imputation models for this.






## Simple imputation model
We first fit a model that is identical to the one in Gómez-Rubio and Rue (2018).

### Specifying priors

```{r}
# Priors for model of interest coefficients
prior.beta = c(0, 0.001) # Gaussian, c(mean, precision)

# Priors for exposure model coefficient
#prior.alpha <- c(26.56, 1/71.07) # Gaussian, c(mean, precision)

# Priors for y, measurement error
prior.prec.y <- c(1, 0.00005) # Gamma
prior.prec.u_c <- c(0.5, 0.5) # Gamma
#prior.prec.x <- c(2.5-1,(2.5)*4.2^2) # Gamma

# Initial values
prec.y <- 1
prec.u_c <- 1
#prec.x <- 1/71.07
prec.x <- 1/71.07
```


### Setting up the matrices for the joint model

```{r}
Y <- matrix(NA, 3*n, 3)

Y[1:n, 1] <- chl             # Regression model of interest response
Y[n+(1:n), 2] <- bmi         # Error model response
Y[2*n+(1:n), 3] <- rep(0, n) # Imputation model response

beta.0 <- c(rep(1, n), rep(NA, n), rep(NA, n))
beta.bmi <- c(1:n, rep(NA, n), rep(NA, n))
beta.age2 <- c(age2, rep(NA, n), rep(NA, n))
beta.age3 <- c(age3, rep(NA, n), rep(NA, n))

id.x <- c(rep(NA, n), 1:n, 1:n) 
weight.x <- c(rep(1, n), rep(1, n), rep(-1, n))

offset.imp <- c(rep(NA, n), rep(NA, n), rep(26.56, n))

dd <- data.frame(Y = Y, 
                 beta.0 = beta.0,
                 beta.bmi = beta.bmi,
                 beta.age2 = beta.age2,
                 beta.age3 = beta.age3,
                 id.x = id.x,
                 weight.x = weight.x,
                 offset.imp = offset.imp)
```

### INLA formula
```{r}
formula = Y ~ - 1 + beta.0 + beta.age2 + beta.age3 + 
  f(beta.bmi, copy="id.x", 
    hyper = list(beta = list(param = prior.beta, fixed=FALSE))) +
  f(id.x, weight.x, model="iid", values = 1:n, 
    hyper = list(prec = list(initial = -15, fixed=TRUE)))
```

### Scaling of ME precision

Since we are not assuming any measurement error here, we need to "turn off" the error model by scaling the error precision to be very large (it makes no difference if we scale the precision only for the observed values or for the observed and missing values).
```{r}
Scale <- c(rep(1, n), rep(10^12, n), rep(1, n))
```

### Fitting the model

```{r}
set.seed(1)
model_missing1 <- inla(formula, data = dd, scale = Scale, offset = log(dat$pop),
                     family = c("gaussian", "gaussian", "gaussian"),
                     control.family = list(
                       list(hyper = list(prec = list(initial = log(prec.y), 
                                                     param = prior.prec.y, 
                                                     fixed = FALSE))),
                       list(hyper = list(prec = list(initial = log(prec.u_c), 
                                                     param = prior.prec.u_c, 
                                                     fixed = FALSE))),
                       list(hyper = list(prec = list(initial = log(prec.x),
                                                     fixed = TRUE)))
                     ),
                     control.fixed = list(
                       mean = list(beta.0 = prior.beta[1], 
                                   beta.age2 = prior.beta[1], 
                                   beta.age3 = prior.beta[1]), 
                       prec = list(beta.0 = prior.beta[2], 
                                   beta.age2 = prior.beta[2], 
                                   beta.age3 = prior.beta[2])),
                     verbose=F)
model_missing1$summary.fixed
model_missing1$summary.hyperpar
```


## Full imputation model
This model includes age as a covariate in the imputation model.

### Specifying priors

For the intercepts and slopes of the model of interest and imputation model we set very wide priors centered at zero. 

```{r}
# Priors for model of interest coefficients
prior.beta = c(0, 1e-6) # Gaussian, c(mean, precision)

# Priors for exposure model coefficients
prior.alpha <- c(0, 1e-6) # Gaussian, c(mean, precision)
```

For the precision of $y$ and $x$ we set more informative priors. The precisions are given gamma priors, and in INLA this is parameterized by the shape and rate parameters. Let's first look at the precision for $y$. 

We base our priors for $x$ and $y$  around the values that we want as the modes of the distributions. For $y$, we look at the regression `chl~bmi+age2+age3`. The standard error is estimated to be $29.1$. We decide we want $1/29.1^2$ to be the mode of our prior. Since the gamma distribution with shape $\alpha$ and rate $\beta$ has mode $\frac{\alpha-1}{\beta}$, we can choose $\alpha = s+1$ and $\beta = s\cdot29.1^2$ in order to get a distribution with mode equal to $1/29.1^2$. Then we need to choose $s$ to decide the spread, the variance of the inverse gamma distribution will decrease as $s$ increases. For $x$ we similarly construct the prior to have its mode at $4.1$.

An alternative approach could be to construct an upper limit for the variance by choosing that to be the variance of the covariate itself, as well as some lower limit, and then select the inverses of these limits to be the 2.7% and 97.5% quantiles in the gamma prior. in our case, the variance of `chl` is $2044$, so we would choose $1/2044$ to be the lower limit for $\tau_y$. For the upper limit, we decide on roughly a 10th of that variation, so the upper limit is $1/204.4$. By specifying these points as the 2.7% and 97.5% quantiles, respectively, we can achieve the parameters of a gamma distribution with these quantiles through numerical optimization. We get the distribution $\tau_y \sim G(3.4, 1588)$. Similarly, for $x$ (`bmi`), we find that the sample variance of `bmi` is $17.8$, and so we could set the lower limit of $\tau_x$ to $1/17.8$. Again, by choosing roughly a 10th of that variation we get a upper limit for the precision at $1/1.78$, giving the gamma distribution $\tau_x \sim G(3.4, 13.9)$.


```{r}
# Priors for y, measurement error and true x-value precision
# Start by getting a reasonable prior guess for the standard error of the regression and exp. models
summary(lm(chl~bmi+age2+age3))$sigma
summary(lm(bmi~age2+age3))$sigma

# Use those values to create reasonable priors:
s <- 0.5
prior.prec.y <- c(s+1, s*29.1^2) # Gamma
prior.prec.x <- c(s+1, s*4.1^2) # Gamma

# We can visualize these priors:
curve(dgamma(x, s+1, s*29.1^2), 0, 0.02) 
abline(v=1/(29.1^2))

curve(dgamma(x, s+1, s*4.1^2), 0, 1)
abline(v=1/(4.2^2))

# Initial values
prec.y <- 1/29.1^2
prec.u_c <- 1
prec.x <- 1/4.2^2
```

### Setting up the matrices for the joint model

```{r}
Y <- matrix(NA, 3*n, 3)


Y[1:n, 1] <- chl             # Regression model of interest response
Y[n+(1:n), 2] <- bmi         # Error model response
Y[2*n+(1:n), 3] <- rep(0, n) # Imputation model response

beta.0 <- c(rep(1, n), rep(NA, n), rep(NA, n))
beta.bmi <- c(1:n, rep(NA, n), rep(NA, n))
beta.age2 <- c(age2, rep(NA, n), rep(NA, n))
beta.age3 <- c(age3, rep(NA, n), rep(NA, n))

id.x <- c(rep(NA, n), 1:n, 1:n) 
weight.x <- c(rep(1, n), rep(1, n), rep(-1, n))

alpha.0 <- c(rep(NA, n), rep(NA, n), rep(1, n))
alpha.age2 <- c(rep(NA, n), rep(NA, n), age2)
alpha.age3 <- c(rep(NA, n), rep(NA, n), age3)

dd <- data.frame(Y = Y, 
                 beta.0 = beta.0,
                 beta.bmi = beta.bmi,
                 beta.age2 = beta.age2,
                 beta.age3 = beta.age3,
                 id.x = id.x,
                 weight.x = weight.x,
                 alpha.0 = alpha.0,
                 alpha.age2 = alpha.age2,
                 alpha.age3 = alpha.age3)
```

### INLA formula
```{r}
formula = Y ~ - 1 + beta.0 + beta.age2 + beta.age3 + 
  f(beta.bmi, copy="id.x", 
    hyper = list(beta = list(param = prior.beta, fixed=FALSE))) +
  f(id.x, weight.x, model="iid", values = 1:n, 
    hyper = list(prec = list(initial = -15, fixed=TRUE))) +
  alpha.0 + alpha.age2 + alpha.age3
```

### Scaling of ME precision

Since we are (again) not assuming any measurement error here, we need to "turn off" the error model by scaling the error precision to be very large (it makes no difference if we scale the precision only for the observed values or for the observed and missing values).
```{r}
Scale <- c(rep(1, n), rep(10^12, n), rep(1, n))
```

### Fitting the model
```{r}
set.seed(1)
model_missing2 <- inla(formula, data = dd, scale = Scale,
                     family = c("gaussian", "gaussian", "gaussian"),
                     control.family = list(
                       list(hyper = list(prec = list(initial = log(prec.y), 
                                                     param = prior.prec.y, 
                                                     fixed = FALSE))),
                       list(hyper = list(prec = list(initial = log(prec.u_c),
                                                     fixed = TRUE))),
                       list(hyper = list(prec = list(initial = log(prec.x), 
                                                     param = prior.prec.x, 
                                                     fixed = FALSE)))
                     ),
                     control.fixed = list(
                       mean = list(beta.0 = prior.beta[1], 
                                   beta.age2 = prior.beta[1], 
                                   beta.age3 = prior.beta[1],  
                                   alpha.0 = prior.alpha[1], 
                                   alpha.age2 = prior.alpha[1],
                                   alpha.age3 = prior.alpha[1]), 
                       prec = list(beta.0 = prior.beta[2], 
                                   beta.age2 = prior.beta[2], 
                                   beta.age3 = prior.beta[2],  
                                   alpha.0 = prior.alpha[2], 
                                   alpha.age2 = prior.alpha[2],
                                   alpha.age3 = prior.alpha[2])),
                     verbose=F)
```


```{r}
#| purl: false
# Save results:
saveRDS(model_missing2, file = "results/model_missing2.rds")
```

## Fitting a complete case model

```{r}
#| purl: false
# Where is bmi missing? 
missing_bmi <- is.na(bmi)

dd_naive <- data.frame(Y = chl, 
                       beta.0 = rep(1, length(bmi)),
                       beta.bmi = bmi, 
                       beta.age2 = age2, 
                       beta.age3 = age3)[!missing_bmi, ]


# Formula
formula <- Y ~ - 1 + beta.0 + beta.age2 + beta.age3 + beta.bmi

# Fit model
set.seed(1)
model_naive <- inla(formula,
              data = dd_naive,
              family = c("gaussian"),
              control.family = list(
                list(hyper = list(prec = list(initial = prec.y, 
                                              param = prior.prec.y, 
                                              fixed = FALSE)))),
              control.fixed = list(
                       mean = list(beta.0 = prior.beta[1], 
                                   beta.age2 = prior.beta[1], 
                                   beta.age3 = prior.beta[1],
                                   beta.bmi = prior.beta[1]), 
                       prec = list(beta.0 = prior.beta[2], 
                                   beta.age2 = prior.beta[2], 
                                   beta.age3 = prior.beta[2],  
                                   beta.bmi = prior.beta[2]))
)
```



## Results

The posterior means and standard deviations are presented in the table below. Note that the data set is quite small (25 observations where 9 are missing), and so the differing result should not be interpreted too seriously.

```{r}
#| echo: false
#| purl: false
return_estimates <- function(inla.object){
  fixed <- data.frame(inla.object$summary.fixed)
  fixed <- fixed[, names(fixed) != "kld"]
  fixed$coefficient.name <- rownames(fixed)
  hyperpar <- data.frame(inla.object$summary.hyperpar)
  hyperpar$coefficient.name <- rownames(hyperpar)

  # Extract all coefficients that contain "beta" (the coefficients of the model of interest)
  betas <- rbind(dplyr::filter(fixed, grepl("beta", coefficient.name)),
                 dplyr::filter(hyperpar, grepl("beta", coefficient.name)))
  # Extract all coefficients that contain "alpha" (the coefficients of the imputation model)
  alphas <- dplyr::filter(fixed, grepl("alpha", coefficient.name))
  
  # All coefficients
  all_coefs <- dplyr::bind_rows("Imputation model" = alphas, 
                                "Model of interest" = betas, 
                                .id = "model")
  
  return(all_coefs)
}
```


```{r}
#| echo: false
#| purl: false
inla_mcmc_results <- tibble::tribble(
  ~coefficient.name, ~model,              ~inla_mcmc.mean, ~inla_mcmc.sd,
  "beta.0",          "Model of interest", 43.469, 62.603,
  "Beta for beta.bmi", "Model of interest", 4.864, 2.206, 
  "beta.age2",         "Model of interest", 29.501, 17.871, 
  "beta.age3",         "Model of interest", 49.449, 23.207
) %>% 
  mutate(inla_mcmc.lower = inla_mcmc.mean-1.96*inla_mcmc.sd, inla_mcmc.upper = inla_mcmc.mean+1.96*inla_mcmc.sd) %>% 
  dplyr::select(-c(inla_mcmc.sd))
  

naive_results <- return_estimates(model_naive) %>%  
  dplyr::select(coefficient.name, mean, X0.025quant, X0.975quant, model) %>% 
  rename(naive.mean = mean, naive.lower = X0.025quant, naive.upper = X0.975quant)
naive_results$coefficient.name[4] <- "Beta for beta.bmi"

return_estimates(model_missing2) %>% 
  dplyr::select(coefficient.name, mean, X0.025quant, X0.975quant, model) %>% 
  rename(me_adjusted.mean = mean, me_adjusted.lower = X0.025quant, me_adjusted.upper = X0.975quant) %>%
  full_join(naive_results, by = c("coefficient.name", "model")) %>% 
  full_join(inla_mcmc_results, by = c("coefficient.name", "model")) %>% 
  mutate_if(is.numeric, round, digits = 3) %>% 
  arrange(desc(model)) %>% # Arrange so the MOI coefs are at the top
  group_by(model) %>% # Group by model type
  gt(rowname_col = "coefficient.name") %>% 
  tab_spanner_delim(
    delim = "."
  )
```

```{r}
#| echo: false
# https://stefvanbuuren.name/RECAPworkshop/Practicals/RECAP_Practical_II.html 
```



```{r}
#| purl: false
#| fig.showtext: true
#| warning: false
#| echo: false
me_plot1 <- return_estimates(model_missing1) %>% 
  dplyr::select(coefficient.name, mean, X0.025quant, X0.975quant, model) %>% 
  dplyr::rename(lower = X0.025quant, upper = X0.975quant)
me_plot2 <- return_estimates(model_missing2) %>% 
  dplyr::select(coefficient.name, mean, X0.025quant, X0.975quant, model) %>% 
  dplyr::rename(lower = X0.025quant, upper = X0.975quant)
naive_plot <- naive_results %>% 
  rename(mean = naive.mean, lower = naive.lower, upper = naive.upper)
inla_mcmc_plot <- inla_mcmc_results %>% 
  rename(mean = inla_mcmc.mean, lower = inla_mcmc.lower, upper = inla_mcmc.upper)

all_models_for_plot <- bind_rows("me1" = me_plot1, 
                                 "me2" = me_plot2,
                                 "naive" = naive_plot, 
                                 "inla_mcmc" = inla_mcmc_plot, 
                                 .id = "model") %>% 
  mutate(coef_name = recode_factor(coefficient.name, "Beta for beta.bmi" = "beta.bmi")) %>%
  filter(coef_name %in% c("beta.age2", "beta.age3", "beta.bmi")) %>% 
  tidyr::separate(coef_name, c("sub_model", "coef_name")) %>% 
  mutate(coef_pretty = paste0(sub_model, "[", coef_name, "]")) %>% 
  mutate(model = fct_relevel(model, "me2", "inla_mcmc", "me1", "naive")) %>% 
  mutate(model = recode(model, "me1" = "Simple imputation model", "me2" = "Full imputation model", "naive" = "Complete case analysis", "inla_mcmc" = "Goméz-Rubio & Rue (2018)")) %>% 
  mutate(coef_pretty = fct_relevel(coef_pretty, "beta[0]", "beta[age2]", "beta[age3]", "beta[bmi]", "alpha[0]", "alpha[age2]", "alpha[age3]"))

showtext_auto()

# Colors
col_bgr <- "white" #"#fbf9f4"
col_text <- "#191919"
color_pal <- c("#004488", "#d6834a", "#DDAA33", "#BB5566")

# Loading fonts
f1 <- "Open Sans"
f2 <- "Open Sans"
font_add_google(name = f1, family = f1)
font_add_google(name = f2, family = f2)

font_size <- 20
# Plot theme
theme_model_summary <- theme_minimal(base_size = font_size, base_family = f1) + 
  theme(
  axis.title.y = element_blank(),
  axis.title.x = element_text(size = 0.7*font_size, color = col_text, family = f1),
  axis.text.y = element_text(size = 0.6*font_size, color = col_text, family = f1),
  axis.text.x = element_text(size = 0.4*font_size, color = col_text, family = f1,
                             margin = margin(0, 0, 1, 0)),
  axis.ticks = element_blank(),
  legend.title = element_blank(),
  panel.background = element_rect(fill = col_bgr, color = col_bgr),
  plot.background = element_rect(fill = col_bgr, color = col_bgr),
  legend.position = "none",
  strip.placement = "outside",
  strip.text = element_text(color = col_text),
  panel.grid.major.y = element_blank(),
  panel.grid.minor = element_blank(),
  panel.border = element_rect(color = "grey65", fill = NA, size = 0.7), 
  plot.title.position = "plot",
  axis.line.x = element_line(size = 1.2, color = "grey65"),
  plot.margin = margin(rep(15, 4))
)

ggplot(all_models_for_plot, aes(y = model)) +
  geom_linerange(aes(xmin = lower, xmax = upper, color = model), size = 1) +
  geom_point(aes(x = mean, color = model), size = 3) +
  geom_text(aes(x = mean, y = model, 
                label = format(round(mean, digits=2), nsmall = 2)), 
            vjust = -1, family = "Open Sans", size = 4) +
  scale_color_manual(values = color_pal) +
  facet_wrap(vars(coef_pretty), nrow = 1,
             labeller = label_parsed, 
             scales = "free_x") +
  labs(x = "Posterior mean") +
  coord_cartesian(clip = "off") +
  theme_model_summary
```


```{r}
#| echo: false
#| warning: false
#| purl: false
ggsave("figures/missing_figure.pdf", height = 4, width = 10, dpi = 600)
ggsave("figures/missing_figure.eps", height = 4, width = 10, dpi = 600, 
       device = cairo_ps)
```

We can also look at the imputed values themselves, they can be found in `marginals.random$id.x` inside the model object.

```{r}
bmi_imputed <- model_missing2$marginals.random$id.x[missing_bmi]
bmi_imp_df <- data.table::rbindlist(lapply(bmi_imputed, as.data.frame), idcol = TRUE)

ggplot(bmi_imp_df, aes(x = x, y = y)) +
  geom_line() +
  facet_wrap(~ .id, ncol = 3) +
  theme_minimal()
```
A summary can also be seen from `summary.random$id.x`:

```{r}
model_missing2$summary.random$id.x[,1:6]
```



### Model summary

```{r}
summary(model_missing2)
```





### References

Gómez-Rubio, V., & Rue, H. (2018). Markov chain Monte Carlo with the integrated nested Laplace approximation. *Statistics and Computing*, 28, 1033–1051. doi: 10.1007/s11222-017-9778-y

