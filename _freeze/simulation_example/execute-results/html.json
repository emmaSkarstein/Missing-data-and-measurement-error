{
  "hash": "21ccab510839d4341391053756a0116c",
  "result": {
    "markdown": "---\ntitle: \"Simulation example\"\nexecute: \n  freeze: auto\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(INLA)\n```\n:::\n\n$$\n\\def\\na{\\texttt{NA}}\n$$\n\n\nWe here provide a detailed guide to the data simulation and measurement error model used in the simulation study in the paper. This vignette goes through the model in great detail, but only one data set is generated. In the example [Simulation study](simulation_study.qmd) the simulation is run 100 times to ensure that the result are not just due to random variation.\n\nFor this example, we simulate a linear regression model with a mismeasured covariate $\\boldsymbol{x}$, observed as $\\boldsymbol{w}$, as well as a covariate without measurement error, $\\boldsymbol{z}$. The covariate $\\boldsymbol{x}$ is constructed to have both Berkson and classical measurement error, and it is also missing (completely at random) approximately 20\\% of the observations.\n\nSome text from the paper: and the respective likelihood functions for each of the sub-models are passed to the function \\texttt{inla()} through the \\texttt{family} argument. To ensure that the shared term across models is understood to be the same in each sub-model, which in our case is the variable $\\boldsymbol{r}$, we use the \\texttt{copy} feature. The \\texttt{copy} feature can be used to directly copy a term that is shared across the sub-models, or the copied effect can be scaled by a parameter. In this case that parameter is simply a constant vector whose terms are either 1 or -1, with the scaling of 1 corresponding to the term in the main model of interest, and the scaling of -1 corresponding to the imputation model, thus ensuring that $\\boldsymbol{x}$ is on the right hand side of the equation, as described in equation \\eqref{eq:trick}.\n\n## Data generation\n\nThe data is generated in the following code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000\n\n# Covariate without error:\nz <- rnorm(n, mean = 0, sd = 1)\n\n# Berkson error:\nw_b <- rnorm(n, mean = 1 + 2*z, sd = 1)\nu_b <- rnorm(n)\nx <- w_b + u_b\n\n# Response:\ny <- 1 + 2*x + 2*z + rnorm(n)\n\n# Classical error:\nu_c <- rnorm(n)\nw_c <- x + u_c  # I think maybe this is incorrect, shouldn't we be using w_b here?\n\n# Missingness:\nm_pred <- -1.5 - 0.5*z # This gives a mean probability of missing of ca 0.2.\nm_prob <- exp(m_pred)/(1 + exp(m_pred))\nm_index <- rbinom(n, 1, prob = m_prob) # MAR\n# m_index <- sample(1:n, 0.2*n, replace = FALSE) # MCAR\nw_c[m_index] <- NA\n\nsimulated_data <- data.frame(y = y, w = w_c, z = z)\n```\n:::\n\n\nThe simulated \"observed\" data then consists of three columns:\n\n\n$$\n\\boldsymbol{y} \\quad \\boldsymbol{w} \\quad \\boldsymbol{z}\n$$\n\n\nFor $n = 1000$ simulated observations, they contain:\n\n  - $y_1, \\dots, y_n$: The continuous response.\n  - $w_1, \\dots, w_n$: A continuous covariate with classical and Berkson measurement error and missing values.\n  - $z_1, \\dots, z_n$: A continuous covariate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(simulated_data)\nn <- nrow(simulated_data)\n```\n:::\n\n\n## Model\nOur response for this model will be\n\n\n$$\ny_i = \\beta_0 + \\beta_x x_i + \\beta_z z_i + \\varepsilon_i \\ , \\quad \\varepsilon_i \\sim N(0, \\tau_y) \\ ,\n$$\n\nthe Berkson error model is\n\n$$\n  x_i = r_i + u_b \\ , \\quad u_b \\sim N(0, \\tau_{u_b}) \\ ,\n$$\n\nthe classical error model is\n\n$$\n  r_i = w_i + u_c \\ , \\quad u_c \\sim N(0, \\tau_{u_c}) \\ ,\n$$\n\nand the imputation model is \n\n$$\nx_i = \\alpha_0 + \\alpha_z z + \\varepsilon_x \\ , \\quad \\varepsilon_x \\sim N(0, \\tau_x)\n$$\n\n\n\nThe prior distributions are\n\n\n  - $\\boldsymbol{x} \\sim N(\\alpha_0 + \\alpha_z \\boldsymbol{z}, \\tau_x \\boldsymbol{I})$,\n  - $\\beta_0, \\beta_x, \\beta_z \\sim N(0, \\tau_{\\beta})$, with $\\tau_{\\beta} = 0.001$,\n  - $\\alpha_0, \\alpha_z \\sim N(0, \\tau_{\\alpha})$, with $\\tau_{\\alpha} = 0.0001$\n  - $\\tau_{y}, \\tau_{u_b}, \\tau_{u_c}, \\tau_{x} \\sim \\text{Gamma}(0.5, 0.5)$,\n\nWe specify the priors in the code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Priors for model of interest coefficients\nprior.beta = c(0, 1/1000) # N(0, 10^3)\n\n# Priors for exposure model coefficients\nprior.alpha <- c(0, 1/10000) # N(0, 10^4)\n  \n# Priors for y, measurement error and true x-value precision\nprior.prec.y <- c(0.5, 0.5) # Gamma(0.5, 0.5)\nprior.prec.u_b <- c(0.5, 0.5) # Gamma(0.5, 0.5)\nprior.prec.u_c <- c(0.5, 0.5) # Gamma(0.5, 0.5)\nprior.prec.x <- c(0.5, 0.5) # Gamma(0.5, 0.5)\n  \n# Initial values\nprec.y <- 1\nprec.u_b <- 1\nprec.u_c <- 1\nprec.x <- 1\n```\n:::\n\n\n\nThe hierarchical model described in the above section is fit in INLA as a joint model using the $\\texttt{copy}$ feature. We first specify the models in the following matrices and vectors:\n\n\n$$\n\\underbrace{\n\\begin{bmatrix}\n  y_1 & \\na & \\na & \\na \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  y_n & \\na & \\na & \\na \\\\\n  \\na &  0  & \\na & \\na \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\na &  0  & \\na & \\na \\\\\n  \\na & \\na & w_1 & \\na \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\na & \\na & w_n & \\na \\\\\n  \\na & \\na & \\na &  0  \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\na & \\na & \\na &  0  \\\\\n\\end{bmatrix}\n}_{\\texttt{Y}}\n=\n\\beta_0\n\\underbrace{\n\\begin{bmatrix}\n1 \\\\\n\\vdots \\\\\n1 \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\end{bmatrix}\n}_{\\texttt{beta.0}}\n+ \\beta_x\n\\underbrace{\n\\begin{bmatrix}\n1 \\\\\n\\vdots \\\\\nn \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\end{bmatrix}\n}_{\\texttt{beta.x}}\n+\n\\underbrace{\n\\begin{bmatrix}\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n-1 \\\\\n\\vdots \\\\\n-n \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n-1 \\\\\n\\vdots \\\\\n-n \\\\\n\\end{bmatrix}\n}_{\\texttt{id.x}}\n+\n\\underbrace{\n\\begin{bmatrix}\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n1 \\\\\n\\vdots \\\\\nn \\\\\n1 \\\\\n\\vdots \\\\\nn \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\end{bmatrix}\n}_{\\texttt{id.r}}\n+ \\beta_z\n\\underbrace{\n\\begin{bmatrix}\nz_1 \\\\\n\\vdots \\\\\nz_n \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\end{bmatrix}\n}_{\\texttt{beta.z}}\n+ \\alpha_0\n\\underbrace{\n\\begin{bmatrix}\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n1 \\\\\n\\vdots \\\\\n1 \\\\\n\\end{bmatrix}\n}_{\\texttt{alpha.0}}\n+ \\alpha_z\n\\underbrace{\n\\begin{bmatrix}\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\nz_1 \\\\\n\\vdots \\\\\nz_n \\\\\n\\end{bmatrix}\n}_{\\texttt{alpha.z}}\n$$\n\n\nWe specify these matrices in our code:\n\n::: {.cell}\n\n```{.r .cell-code}\nY <- matrix(NA, 4*n, 4)\n\nY[1:n, 1] <- y               # Regression model of interest response\nY[n+(1:n), 2] <- rep(0, n)   # Berkson error model response\nY[2*n+(1:n), 3] <- w         # Classical error model response\nY[3*n+(1:n), 4] <- rep(0, n) # Imputation model response\n\nbeta.0 <- c(rep(1, n), rep(NA, 3*n))\nbeta.x <- c(1:n, rep(NA, 3*n))\nbeta.z <- c(z, rep(NA, 3*n))\n\nid.x <- c(rep(NA, n), 1:n, rep(NA, n), 1:n)\nweight.x <- c(rep(1, n), rep(-1, n), rep(1, n), rep(-1, n))\n\nbeta.r <- c(rep(NA, n), 1:n, 1:n, rep(NA, n))\nweight.r <- c(rep(1, 4*n))\n\nalpha.0 = c(rep(NA, 3*n), rep(1, n))\nalpha.z = c(rep(NA, 3*n), z)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndd <- data.frame(Y = Y,\n                 beta.0 = beta.0,\n                 beta.x = beta.x,\n                 beta.z = beta.z,\n                 id.x = id.x, \n                 weight.x = weight.x,\n                 beta.r = beta.r,\n                 weight.r = weight.r,\n                 alpha.0 = alpha.0,\n                 alpha.z = alpha.z)\n```\n:::\n\n\n\n\nNext, we set up the INLA formula. There are four fixed effects ($\\beta_0$, $\\beta_z$, $\\alpha_0$, $\\alpha_z$) and three random effects. Two of the random effects are necessary to ensure that the values of $\\boldsymbol{r}$ are the same in the exposure model and error model are assigned the same values as in the regression model, where $\\beta_x \\boldsymbol{r}$ is the product of two unknown quantities. The third random effect term is for encoding the Berkson error model.\n\n\n  - `f(beta.x, copy=\"id.x\", ...)`: The `copy=\"id.x\"` argument ensures that identical values are assigned to $\\boldsymbol{x}$ in all components of the joint model. $\\beta_x$, which is treated as a hyperparameter, is the scaling parameter of the copied process $\\boldsymbol{x}^*$.\n  - `f(id.x, weight.x, ...)`: `id.x` contains the $\\boldsymbol{x}$-values, encoded as an i.i.d. Gaussian random effect, and weighted with `weight.x` to ensure the correct signs in the joint model. The `values` option contains the vector of all values assumes by the covariate for which the effect is estimated. The precision `prec` of the random effect is fixed at $\\exp(-15)$, which is necessary since the uncertainty in $\\boldsymbol{x}$ is already modeled in the second level (column 2 of `Y`) of the joint model, which defines the imputation component.\n  - `f(beta.r, weight.r, ...)`: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula = Y ~ - 1 + beta.0 + beta.z +\n  f(beta.x, copy = \"id.x\",  \n    hyper = list(beta = list(param = prior.beta, fixed = FALSE))) +\n  f(id.x, weight.x, model = \"iid\", values = 1:n, \n    hyper = list(prec = list(initial = -15, fixed = TRUE))) +\n  f(beta.r, weight.r, model=\"iid\", values = 1:n, \n    hyper = list(prec = list(initial = -15, fixed = TRUE))) + \n#  f(u.b.tilde, model = \"iid\", values = 1:n,\n#    hyper = list(prec = list(initial = log(1), fixed=TRUE))) +\n  alpha.0 + alpha.z\n```\n:::\n\n\n\nWe explicitly remove the intercept using `-1` since there is no common intercept in the joint model, and the model specific intercepts $\\beta_0$ and $\\alpha_0$ are specified instead.\n\nNext comes the call of the `inla` function. We explain further some of the terms:\n\n  - `family`: Here we need to specify one likelihood function for each of the model levels corresponding to each column in the matrix `Y`. In this case, they are all Gaussian, but if we for instance had a logistic regression model as our model of interest, then the list would be `c(\"binomial\", \"gaussian\", \"gaussian\")`.\n  - `control.family`: Here we specify the hyperparameters for each of the three likelihoods. In this case, we specify the precision for each Gaussian likelihood, $\\tau_y$, $\\tau_{u_b}$, $\\tau_{u_c}$ and $\\tau_{x}$, respectively.\n  - `control.fixed`: Prior specification for the fixed effects.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_sim <- inla(formula, data = dd, scale = scale.vec,\n                  family = c(\"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\"),\n                  control.family = list(\n                    list(hyper = list(prec = list(initial = log(prec.y),\n                                                  param = prior.prec.y,\n                                                  fixed = FALSE))),\n                    list(hyper = list(prec = list(initial = log(prec.u_b),\n                                                  param = prior.prec.u_b,\n                                                  fixed = TRUE))),\n                    list(hyper = list(prec = list(initial = log(prec.u_c),\n                                                  param = prior.prec.u_c,\n                                                  fixed = TRUE))),\n                    list(hyper = list(prec = list(initial = log(prec.x),\n                                                  param = prior.prec.x,\n                                                  fixed = FALSE)))\n                  ),\n                  control.fixed = list(\n                    mean = list(beta.0 = prior.beta[1],\n                                beta.z = prior.beta[1],\n                                alpha.0 = prior.alpha[1],\n                                alpha.z = prior.alpha[1]),\n                    prec = list(beta.0 = prior.beta[2],\n                                beta.z = prior.beta[2],\n                                alpha.0 = prior.alpha[2],\n                                alpha.z = prior.alpha[2]))\n               )\n```\n:::\n\n\n## Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary of fixed effects:\nmodel_sim$summary.fixed[1:2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             mean         sd\nbeta.0  0.6504279 0.13360447\nbeta.z  1.2022995 0.20374379\nalpha.0 0.9936363 0.05961909\nalpha.z 2.0786850 0.05952287\n```\n:::\n\n```{.r .cell-code}\n# Summary of random effects:\nmodel_sim$summary.hyperpar[1:2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                mean         sd\nPrecision for the Gaussian observations    7.2369772 2.88312242\nPrecision for the Gaussian observations[4] 0.6552417 0.05596101\nBeta for beta.x                            2.3094190 0.08384926\n```\n:::\n:::\n\n\nThe fixed effects can then be accessed through `model$summary.fixed`, whereas the posterior mean and sd for the coefficient of $\\boldsymbol{x}$ can be accessed through `model$summary.hyperpar`. [todo: what are the two precisions? The precision for the MOI and the imputation model?]\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save the INLA-model to the package so it can be summarized in the paper.\nsaveRDS(model_sim, file = \"results/model_simulation.rds\")\n```\n:::\n",
    "supporting": [
      "simulation_example_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}