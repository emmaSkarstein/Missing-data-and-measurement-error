{
  "hash": "57578af2272c539539e3a58e3b544cc2",
  "result": {
    "markdown": "---\ntitle: \"Simulation example\"\nexecute: \n  freeze: true\nknitr:\n  opts_chunk: \n    message: false\n---\n\n\n\n$$\n\\def\\na{\\texttt{NA}}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(INLA)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nWe here provide a detailed guide to the data simulation and measurement error model used in the simulation study in the paper. This vignette goes through the model in great detail, but only one data set is generated. In the example [Simulation study](simulation_study.qmd) the simulation is run 100 times to ensure that the result are not just due to random variation.\n\nFor this example, we simulate a linear regression model with a mismeasured covariate $\\boldsymbol{x}$, observed as $\\boldsymbol{w}$, as well as a covariate without measurement error, $\\boldsymbol{z}$. The covariate $\\boldsymbol{x}$ is constructed to have both Berkson and classical measurement error, and it is also missing (completely at random) approximately 20\\% of the observations.\n\n## Data generation\n\nThe data is generated in the following code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2022)\nn <- 1000\n\n# Covariate without error:\nz <- rnorm(n, mean = 0, sd = 1)\n\n# Berkson error:\nu_b <- rnorm(n, sd = 1)\nr <- rnorm(n, mean = 1 + 2*z, sd = 1)\nx <- r + u_b\n\n# Response:\ny <- 1 + 2*x + 2*z + rnorm(n)\n\n# Classical error:\nu_c <- rnorm(n, sd = 1)\nw <- r + u_c \n\n# Missingness:\nm_pred <- -1.5 - 0.5*z # This gives a mean probability of missing of ca 0.2.\nm_prob <- exp(m_pred)/(1 + exp(m_pred))\n\nm_index <- as.logical(rbinom(n, 1, prob = m_prob)) # MAR\n# m_index <- sample(1:n, 0.2*n, replace = FALSE) # MCAR\nw[m_index] <- NA\n\nsimulated_data <- data.frame(y = y, w = w, z = z)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nThe simulated \"observed\" data then consists of three columns:\n\n$$\n\\boldsymbol{y} \\quad \\boldsymbol{w} \\quad \\boldsymbol{z}\n$$\n\nFor $n = 1000$ simulated observations, they contain:\n\n  - $y_1, \\dots, y_n$: The continuous response.\n  - $w_1, \\dots, w_n$: A continuous covariate with classical and Berkson measurement error and missing values.\n  - $z_1, \\dots, z_n$: A continuous covariate without measurement error or missingness.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(simulated_data)\nn <- nrow(simulated_data)\n```\n:::\n\n\n## Model\nOur response for this model will be\n\n$$\n\\boldsymbol{y} = \\beta_0 + \\beta_x \\boldsymbol{x} + \\beta_z \\boldsymbol{z} + \\boldsymbol{\\varepsilon} \\ , \\quad \\boldsymbol{\\varepsilon} \\sim N(\\boldsymbol{0}, \\tau_y\\boldsymbol{I}) \\ ,\n$$\nthe Berkson error model is\n$$\n  \\boldsymbol{x} = \\boldsymbol{r} + \\boldsymbol{u}_b \\ , \\quad \\boldsymbol{u}_b \\sim N(\\boldsymbol{0}, \\tau_{u_b}\\boldsymbol{I}) \\ ,\n$$\nthe classical error model is\n$$\n  \\boldsymbol{w} = \\boldsymbol{r} + \\boldsymbol{u}_c \\ , \\quad \\boldsymbol{u}_c \\sim N(\\boldsymbol{0}, \\tau_{u_c}\\boldsymbol{I}) \\ ,\n$$\nand the imputation model is \n$$\n\\boldsymbol{r} = \\alpha_0 + \\alpha_z \\boldsymbol{z} + \\boldsymbol{\\varepsilon}_r \\ , \\quad \\boldsymbol{\\varepsilon}_r \\sim N(\\boldsymbol{0}, \\tau_r\\boldsymbol{I}) \\ .\n$$\nRewritten for INLA these models are\n$$\n\\begin{align}\n  \\boldsymbol{y} &= \\beta_0 + \\beta_x \\boldsymbol{x} + \\beta_z \\boldsymbol{z} + \\boldsymbol{\\varepsilon} \\ , \\quad &\\boldsymbol{\\varepsilon} \\sim N(\\boldsymbol{0}, \\tau_y\\boldsymbol{I}) \\ , \\\\\n  \\boldsymbol{0} &= -\\boldsymbol{x} + \\boldsymbol{r} + \\boldsymbol{u}_b \\ , \\quad & \\boldsymbol{u}_b \\sim N(\\boldsymbol{0}, \\tau_{u_b}\\boldsymbol{I}) \\ , \\\\\n  \\boldsymbol{w} &= \\boldsymbol{r} + \\boldsymbol{u}_c \\ , \\quad &\\boldsymbol{u}_c \\sim N(\\boldsymbol{0}, \\tau_{u_c}\\boldsymbol{I}) \\ , \\\\\n  \\boldsymbol{0} &= -\\boldsymbol{r} + \\alpha_0 + \\alpha_z \\boldsymbol{z} + \\boldsymbol{\\varepsilon}_r \\ , \\quad &\\boldsymbol{\\varepsilon}_r \\sim N(\\boldsymbol{0}, \\tau_r\\boldsymbol{I}) \\ .\n\\end{align}\n$$\n\nThe prior distributions are\n\n\n  - $\\boldsymbol{r} \\sim N(\\alpha_0 + \\alpha_z \\boldsymbol{z}, \\tau_r \\boldsymbol{I})$,\n  - $\\beta_0, \\beta_x, \\beta_z \\sim N(0, \\tau_{\\beta})$, with $\\tau_{\\beta} = 0.001$,\n  - $\\alpha_0, \\alpha_z \\sim N(0, \\tau_{\\alpha})$, with $\\tau_{\\alpha} = 0.0001$\n  - $\\tau_{y}, \\tau_{u_b}, \\tau_{u_c}, \\tau_{r} \\sim \\text{Gamma}(0.5, 0.5)$,\n\nWe specify the priors in the code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Priors for model of interest coefficients\nprior.beta <- c(0, 1/1000) # N(0, 10^3)\n\n# Priors for exposure model coefficients\nprior.alpha <- c(0, 1/10000) # N(0, 10^4)\n  \n# Priors for y, measurement error and true x-value precision\nprior.prec.y <- c(0.5, 0.5) # Gamma(0.5, 0.5)\nprior.prec.u_b <- c(0.5, 0.5) # Gamma(0.5, 0.5)\nprior.prec.u_c <- c(0.5, 0.5) # Gamma(0.5, 0.5)\nprior.prec.r <- c(0.5, 0.5) # Gamma(0.5, 0.5)\n  \n# Initial values\nprec.y <- 1\nprec.u_b <- 1\nprec.u_c <- 1\nprec.r <- 1\n```\n:::\n\n\n\nThe hierarchical model described in the above section is fit in INLA as a joint model using the $\\texttt{copy}$ feature. We first specify the models in the following matrices and vectors:\n\n$$\n\\underbrace{\n\\begin{bmatrix}\n  y_1 & \\na & \\na & \\na \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  y_n & \\na & \\na & \\na \\\\\n  \\na &  0  & \\na & \\na \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\na &  0  & \\na & \\na \\\\\n  \\na & \\na & w_1 & \\na \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\na & \\na & w_n & \\na \\\\\n  \\na & \\na & \\na &  0  \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  \\na & \\na & \\na &  0  \\\\\n\\end{bmatrix}\n}_{\\texttt{Y}}\n=\n\\beta_0\n\\underbrace{\n\\begin{bmatrix}\n1 \\\\\n\\vdots \\\\\n1 \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\end{bmatrix}\n}_{\\texttt{beta.0}}\n+ \\beta_x\n\\underbrace{\n\\begin{bmatrix}\n1 \\\\\n\\vdots \\\\\nn \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\end{bmatrix}\n}_{\\texttt{beta.x}}\n+\n\\underbrace{\n\\begin{bmatrix}\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n-1 \\\\\n\\vdots \\\\\n-n \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\end{bmatrix}\n}_{\\texttt{id.x}}\n+\n\\underbrace{\n\\begin{bmatrix}\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n1 \\\\\n\\vdots \\\\\nn \\\\\n1 \\\\\n\\vdots \\\\\nn \\\\\n-1 \\\\\n\\vdots \\\\\n-n \\\\\n\\end{bmatrix}\n}_{\\texttt{id.r}}\n+ \\beta_z\n\\underbrace{\n\\begin{bmatrix}\nz_1 \\\\\n\\vdots \\\\\nz_n \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\end{bmatrix}\n}_{\\texttt{beta.z}}\n+ \\alpha_0\n\\underbrace{\n\\begin{bmatrix}\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n1 \\\\\n\\vdots \\\\\n1 \\\\\n\\end{bmatrix}\n}_{\\texttt{alpha.0}}\n+ \\alpha_z\n\\underbrace{\n\\begin{bmatrix}\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\n\\na \\\\\n\\vdots \\\\\n\\na \\\\\nz_1 \\\\\n\\vdots \\\\\nz_n \\\\\n\\end{bmatrix}\n}_{\\texttt{alpha.z}}\n$$\n\nWe specify these matrices in our code:\n\n::: {.cell}\n\n```{.r .cell-code}\nY <- matrix(NA, 4*n, 4)\n\nY[1:n, 1] <- y                   # Regression model of interest response\nY[n+(1:n), 2] <- rep(0, n)       # Berkson error model response\nY[2*n+(1:n), 3] <- w             # Classical error model response\nY[3*n+(1:n), 4] <- rep(0, n)     # Imputation model response\n\nbeta.0 <- c(rep(1, n), rep(NA, 3*n))\nbeta.x <- c(1:n, rep(NA, 3*n))\nbeta.z <- c(z, rep(NA, 3*n))\n\nid.x <- c(rep(NA, n), 1:n, rep(NA, n), rep(NA, n))\nweight.x <- c(rep(NA, n), rep(-1, n), rep(NA, n), rep(NA, n))\n\nid.r <- c(rep(NA, n), 1:n, 1:n, 1:n)\nweight.r <- c(rep(NA, n), rep(1, n), rep(1, n), rep(-1, n))\n\nalpha.0 <- c(rep(NA, 3*n), rep(1, n))\nalpha.z <- c(rep(NA, 3*n), z)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndd <- list(Y = Y,\n           beta.0 = beta.0,\n           beta.x = beta.x,\n           beta.z = beta.z,\n           id.x = id.x, \n           weight.x = weight.x,\n           id.r = id.r,\n           weight.r = weight.r,\n           alpha.0 = alpha.0,\n           alpha.z = alpha.z)\n```\n:::\n\n\n\n\nNext, we set up the INLA formula. There are four fixed effects ($\\beta_0$, $\\beta_z$, $\\alpha_0$, $\\alpha_z$) and three random effects. Two of the random effects are necessary to ensure that the values of $\\boldsymbol{r}$ are the same in the exposure model and error model are assigned the same values as in the regression model, where $\\beta_x \\boldsymbol{r}$ is the product of two unknown quantities. The third random effect term is for encoding the Berkson error model.\n\n\n  - `f(beta.x, copy=\"id.x\", ...)`: The `copy=\"id.x\"` argument ensures that identical values are assigned to $\\boldsymbol{x}$ in all components of the joint model. $\\beta_x$, which is treated as a hyperparameter, is the scaling parameter of the copied process $\\boldsymbol{x}^*$.\n  - `f(id.x, weight.x, ...)`: `id.x` contains the $\\boldsymbol{x}$-values, encoded as an i.i.d. Gaussian random effect, and weighted with `weight.x` to ensure the correct signs in the joint model. The `values` option contains the vector of all values assumes by the covariate for which the effect is estimated. The precision `prec` of the random effect is fixed at $\\exp(-15)$, which is necessary since the uncertainty in $\\boldsymbol{x}$ is already modeled in the second level (column 2 of `Y`) of the joint model, which defines the imputation component.\n  - `f(id.r, weight.r, ...)`: in the same way that `id.x`, contains the $\\boldsymbol{x}$-values, `id.r` contains the $\\boldsymbol{r}$-values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula <- Y ~ - 1 + beta.0 + beta.z +\n  f(beta.x, copy = \"id.x\",  \n    hyper = list(beta = list(param = prior.beta, fixed = FALSE))) +\n  f(id.x, weight.x, model = \"iid\", values = 1:n, \n    hyper = list(prec = list(initial = -15, fixed = TRUE))) +\n  f(id.r, weight.r, model=\"iid\", values = 1:n, \n    hyper = list(prec = list(initial = -15, fixed = TRUE))) + \n  alpha.0 + alpha.z\n```\n:::\n\n\n\nWe explicitly remove the intercept using `-1` since there is no common intercept in the joint model, and the model specific intercepts $\\beta_0$ and $\\alpha_0$ are specified instead.\n\nNext comes the call of the `inla` function. We explain further some of the terms:\n\n  - `family`: Here we need to specify one likelihood function for each of the model levels corresponding to each column in the matrix `Y`. In this case, they are all Gaussian, but if we for instance had a logistic regression model as our model of interest, then the list would be `c(\"binomial\", \"gaussian\", \"gaussian\", \"gaussian\")`.\n  - `control.family`: Here we specify the hyperparameters for each of the three likelihoods. In this case, we specify the precision for each Gaussian likelihood, $\\tau_y$, $\\tau_{u_b}$, $\\tau_{u_c}$ and $\\tau_{r}$, respectively.\n  - `control.predictor`: Computes the predictive distribution of the missing observations in the response.\n  - `control.fixed`: Prior specification for the fixed effects.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_sim <- inla(formula, data = dd,\n                  family = c(\"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\"),\n                  control.family = list(\n                    list(hyper = list(prec = list(initial = log(prec.y),\n                                                  param = prior.prec.y,\n                                                  fixed = FALSE))),\n                    list(hyper = list(prec = list(initial = log(prec.u_b),\n                                                  param = prior.prec.u_b,\n                                                  fixed = FALSE))),\n                    list(hyper = list(prec = list(initial = log(prec.u_c),\n                                                  param = prior.prec.u_c,\n                                                  fixed = FALSE))),\n                    list(hyper = list(prec = list(initial = log(prec.r),\n                                                  param = prior.prec.r,\n                                                  fixed = FALSE)))\n                  ),\n                  control.predictor = list(compute = TRUE), \n                  control.fixed = list(\n                    mean = list(beta.0 = prior.beta[1],\n                                beta.z = prior.beta[1],\n                                alpha.0 = prior.alpha[1],\n                                alpha.z = prior.alpha[1]),\n                    prec = list(beta.0 = prior.beta[2],\n                                beta.z = prior.beta[2],\n                                alpha.0 = prior.alpha[2],\n                                alpha.z = prior.alpha[2]))\n               )\n```\n:::\n\n\n## Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary of fixed effects:\nfixed <- model_sim$summary.fixed[1:5]\nfixed[c(\"mean\", \"0.025quant\", \"0.975quant\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            mean 0.025quant 0.975quant\nbeta.0  1.045890  0.6562144   1.372191\nbeta.z  2.157892  1.4764692   2.676830\nalpha.0 1.005302  0.9123583   1.098251\nalpha.z 1.987672  1.8916031   2.083756\n```\n:::\n\n```{.r .cell-code}\n# Summary of random effects:\nhyper <- model_sim$summary.hyperpar[1:5]\nhyper[c(\"mean\", \"0.025quant\", \"0.975quant\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                mean 0.025quant 0.975quant\nPrecision for the Gaussian observations    0.6573104  0.4748465  0.6681521\nPrecision for the Gaussian observations[2] 1.4160039  1.1534509 11.2367339\nPrecision for the Gaussian observations[3] 1.0216925  0.9021679  1.0810789\nPrecision for the Gaussian observations[4] 1.0364528  0.9792048  1.9582454\nBeta for beta.x                            2.1149641  2.0095097  3.2729965\n```\n:::\n:::\n\n\nThe fixed effects can then be accessed through `model$summary.fixed`, whereas the posterior mean and sd for the coefficient of $\\boldsymbol{x}$ can be accessed through `model$summary.hyperpar`, since $\\beta_x$ is actually a hyperparameter of the model. In `model$summary.hyperpar` we also get the precision terms for each of the sub-models in the order they have been defined, so the first precision is $\\tau_y$, the second one $\\tau_{u_b}$, the third one $\\tau_{u_c}$ and the final one is $\\tau_r$.\n\n\n\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "simulation_example_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}