{
  "hash": "12e80336e660b7f08b4bc3b3831c3a20",
  "result": {
    "markdown": "---\ntitle: \"Simulation study\"\nexecute: \n  freeze: true\nknitr:\n  opts_chunk: \n    message: false\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(tidyverse)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nIn the vignette [Simulation example](Simulation_example.qmd), we simulate a single data set with Berkson error, classical error and missing data, and then fit a measurement error model to adjust for these errors. In this simulation study, we do the exact same steps, but repeated on 100 simulated data sets instead of just one, to ensure that the results are not an artifact of one particular data set. This vignette consists of mostly just code, for detailed explanations on the steps taken in the analysis, please refer to [Simulation example](Simulation_example.qmd).\n\n\n## Setting up functions\n\n### Function for simulating data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_data <- function(n){\n  # Covariate without error:\n  z <- rnorm(n, mean = 0, sd = 1)\n  \n  # Berkson error:\n  u_b <- rnorm(n)\n  r <- rnorm(n, mean = 1 + 2*z, sd = 1)\n  x <- r + u_b\n  \n  # Response:\n  y <- 1 + 2*x + 2*z + rnorm(n)\n  \n  # Classical error:\n  u_c <- rnorm(n)\n  w <- r + u_c \n  \n  # Missingness:\n  m_pred <- -1.5 - 0.5*z # MAR. This gives a mean probability of missing of ca 0.2.\n  # m_pred <- -1.5 - 0.5*x # MNAR\n  m_prob <- exp(m_pred)/(1 + exp(m_pred))\n  m_index <- as.logical(rbinom(n, 1, prob = m_prob)) # MAR/MNAR\n  # m_index <- sample(1:n, 0.2*n, replace = FALSE) # MCAR\n  w[m_index] <- NA\n\n  simulated_data <- data.frame(y = y, w = w, z = z, x = x)\n  return(simulated_data)\n}\n```\n:::\n\n\n### Functions for setting up the model matrices\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make matrix for ME model\nmake_matrix_ME <- function(data){\n  n <- nrow(data)\n  \n  y <- data$y\n  w <- data$w\n  z <- data$z\n  \n  Y <- matrix(NA, 4*n, 4)\n\n  Y[1:n, 1] <- y                 # Regression model of interest response\n  Y[n+(1:n), 2] <- rep(0, n)     # Berkson error model response\n  Y[2*n+(1:n), 3] <- w           # Classical error model response\n  Y[3*n+(1:n), 4] <- rep(0, n)   # Imputation model response\n\n  beta.0 <- c(rep(1, n), rep(NA, 3*n))\n  beta.x <- c(1:n, rep(NA, 3*n))\n  beta.z <- c(z, rep(NA, 3*n))\n\n  id.x <- c(rep(NA, n), 1:n, rep(NA, n), rep(NA, n))\n  weight.x <- c(rep(NA, n), rep(-1, n), rep(NA, n), rep(NA, n))\n\n  id.r <- c(rep(NA, n), 1:n, 1:n, 1:n)\n  weight.r <- c(rep(NA, n), rep(1, n), rep(1, n), rep(-1, n))\n\n  alpha.0 = c(rep(NA, 3*n), rep(1, n))\n  alpha.z = c(rep(NA, 3*n), z)\n  \n  dd_adj <- list(Y = Y,\n                       beta.0 = beta.0,\n                       beta.x = beta.x,\n                       beta.z = beta.z,\n                       id.x = id.x, \n                       weight.x = weight.x,\n                       id.r = id.r,\n                       weight.r = weight.r,\n                       alpha.0 = alpha.0,\n                       alpha.z = alpha.z)\n\n  return(dd_adj)\n}\n\n# Make matrix for naive model\nmake_matrix_naive <- function(data){\n  y <- data$y\n  w <- data$w\n  z <- data$z\n  \n  # Naive model\n  dd_naive <- list(Y = y,\n                         beta.0 = rep(1, nrow(data)),\n                         beta.x = w, \n                         beta.z = z)\n  return(dd_naive)\n}\n\n# Make matrix for model using the unobserved variable\nmake_matrix_true <- function(data){\n  y <- data$y\n  x <- data$x\n  z <- data$z\n  # True model\n  dd_naive <- list(Y = y,\n                         beta.0 = rep(1, nrow(data)),\n                         beta.x = x, \n                         beta.z = z)\n}\n```\n:::\n\n\n\n### Function for fitting the ME model\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit ME model\nfit_model_ME <- function(data_matrix) {\n  # Priors for model of interest coefficients\n  prior.beta <- c(0, 1/1000) # N(0, 10^3)\n  \n  # Priors for exposure model coefficients\n  prior.alpha <- c(0, 1/10000) # N(0, 10^4)\n  \n  # Priors for y, measurement error and true x-value precision\n  prior.prec.y <- c(0.5, 0.5) # Gamma(0.5, 0.5)\n  prior.prec.u_b <- c(0.5, 0.5) # Gamma(0.5, 0.5)\n  prior.prec.u_c <- c(0.5, 0.5) # Gamma(0.5, 0.5)\n  prior.prec.r <- c(0.5, 0.5) # Gamma(0.5, 0.5)\n  \n  # Initial values\n  prec.y <- 1\n  prec.u_b <- 1\n  prec.u_c <- 1\n  prec.r <- 1\n  \n  # Formula\n  formula = Y ~ - 1 + beta.0 + beta.z +\n    f(beta.x, copy = \"id.x\",  \n      hyper = list(beta = list(param = prior.beta, fixed = FALSE))) +\n    f(id.x, weight.x, model = \"iid\", values = 1:n, \n      hyper = list(prec = list(initial = -15, fixed = TRUE))) +\n    f(id.r, weight.r, model=\"iid\", values = 1:n, \n      hyper = list(prec = list(initial = -15, fixed = TRUE))) + \n    alpha.0 + alpha.z\n  \n  # Fit model\n  model <- inla(formula,\n                data = data_matrix,\n                family = c(\"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\"),\n                control.family = list(\n                  list(hyper = list(prec = list(initial = log(prec.y), \n                                                param = prior.prec.y, \n                                                fixed = FALSE))), \n                  list(hyper = list(prec = list(initial = log(prec.u_b),\n                                                param = prior.prec.u_b,\n                                                fixed = FALSE))),\n                  list(hyper = list(prec = list(initial = log(prec.u_c), \n                                                param = prior.prec.u_c, \n                                                fixed = FALSE))), \n                  list(hyper = list(prec = list(initial = log(prec.r), \n                                                param = prior.prec.r, \n                                                fixed = FALSE)))), \n                control.predictor = list(compute = TRUE), \n                control.fixed = list(\n                  mean = list(\n                    beta.0 = prior.beta[1],\n                    beta.z = prior.beta[1],\n                    alpha.0 = prior.alpha[1],\n                    alpha.z = prior.alpha[1]),\n                  prec = list(\n                    beta.0 = prior.beta[2],\n                    beta.z = prior.beta[2],\n                    alpha.0 = prior.alpha[2],\n                    alpha.z = prior.alpha[2])\n    )\n  )\n}\n```\n:::\n\n\n### Function for fitting the true/naive model\n\nThe same function can be used to fit the naive model (`y ~ w + z`) and the best-case model (`y ~ x + z`) since they simply differ in the variable that is inputted (`w` versus `x`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_model_naive_true <- function(data_matrix){\n  # Priors for model of interest coefficients\n  prior.beta <- c(0, 1/1000) # N(0, 10^3)\n\n  # Priors for y, measurement error and true x-value precision\n  prior.prec.y <- c(0.5, 0.5) # Gamma(0.5, 0.5)\n  \n  # Initial values\n  prec.y <- 1\n\n  # Formula\n  formula <- Y ~ beta.0 - 1 + beta.x + beta.z\n  \n  # Fit model\n  model <- inla(formula,\n                data = data_matrix,\n                family = c(\"gaussian\"),\n                control.family = list(\n                  list(hyper = list(prec = list(initial = log(prec.y), \n                                                param = prior.prec.y, \n                                                fixed = FALSE)))),\n                control.fixed = list(\n                  mean = list(\n                    beta.0 = prior.beta[1],\n                    beta.z = prior.beta[1],\n                    beta.x = prior.beta[1]),\n                  prec = list(\n                    beta.0 = prior.beta[2],\n                    beta.z = prior.beta[2],\n                    beta.x = prior.beta[2])\n    )\n  )\n}\n```\n:::\n\n\n\n\n## Fitting the model for each data set\n\nWe simulate 100 data sets and fit the model that accounts for measurement error and missing data, and then save the posterior means for the intercept ans slopes.\n\nNote that this chunk may take a while to run.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\n# Number of iterations\nniter <- 100\n\n# Data frames to store the results \nresults_ME <- data.frame(matrix(NA, nrow=niter, ncol=5))\nnames(results_ME) <- c(\"beta.0\", \"beta.x\", \"beta.z\", \"alpha.0\", \"alpha.z\")\n\nresults_naive <- data.frame(matrix(NA, nrow=niter, ncol=3))\nnames(results_naive) <- c(\"beta.0\", \"beta.x\", \"beta.z\")\n\nresults_true <- data.frame(matrix(NA, nrow=niter, ncol=3))\nnames(results_true) <- c(\"beta.0\", \"beta.x\", \"beta.z\")\n\n\nfor(i in 1:niter){\n  n <- 1000\n  data <- simulate_data(n)\n  \n  # ME model\n  matrix_ME <- make_matrix_ME(data)\n  model_ME <- fit_model_ME(matrix_ME)\n  \n  # Naive model\n  matrix_naive <- make_matrix_naive(data)\n  model_naive <- fit_model_naive_true(matrix_naive)\n  \n  # True model\n  matrix_true <- make_matrix_true(data)\n  model_true <- fit_model_naive_true(matrix_true)\n  \n  results_ME[i, c(\"beta.0\", \"beta.z\", \n                  \"alpha.0\", \"alpha.z\")] <- t(model_ME$summary.fixed[\"mean\"])\n  results_ME[i, \"beta.x\"] <- model_ME$summary.hyperpar[\"Beta for beta.x\", \"mean\"]\n  \n  results_naive[i, c(\"beta.0\", \"beta.x\", \"beta.z\")] <- t(model_naive$summary.fixed[\"mean\"])\n  \n  results_true[i, c(\"beta.0\", \"beta.x\", \"beta.z\")] <- t(model_true$summary.fixed[\"mean\"])\n\n}\n```\n:::\n\n\n## Results\n\n\n::: {.cell fig.showtext='true'}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(joint_results, file = \"results/simulation_results.rds\")\n```\n:::\n\n::: {.cell fig.showtext='true'}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(colorspace)\nlibrary(showtext)\nshowtext_auto()\n\n# Colors\ncol_bgr <- \"white\" #\"#fbf9f4\"\ncol_text <- \"#191919\"\ncolor_pal <- c(\"#004488\", \"#DDAA33\", \"#BB5566\")\n\n# Loading fonts\nf1 <- \"Open Sans\"\nfont_add_google(name = f1, family = f1)\n\nfont_size <- 20\n# Plot theme\ntheme_model_summary <- theme_minimal(base_size = font_size, base_family = f1) + \n  theme(\n  axis.title.y = element_blank(),\n  axis.title.x = element_text(size = 0.7*font_size, color = col_text, family = f1),\n  axis.text.y = element_text(size = 0.6*font_size, color = col_text, family = f1),\n  axis.text.x = element_text(size = 0.4*font_size, color = col_text, family = f1,\n                             margin = margin(0, 0, 1, 0)),\n  axis.ticks = element_blank(),\n  legend.title = element_blank(),\n  panel.background = element_rect(fill = col_bgr, color = col_bgr),\n  plot.background = element_rect(fill = col_bgr, color = col_bgr),\n  legend.position = \"none\",\n  strip.placement = \"outside\",\n  strip.text = element_text(color = col_text),\n  panel.grid.major.y = element_blank(),\n  panel.grid.minor.y = element_blank(),\n  panel.border = element_rect(color = \"grey65\", fill = NA, linewidth = 1), #\n  plot.title.position = \"plot\",\n  axis.line.x = element_line(linewidth = 1, color = \"grey65\"),#\n  plot.margin = margin(rep(15, 4))\n)\n\nggplot(simulation_results, aes(x = value, y = model, color = model)) +\n  # Invisible points to set limits\n  #geom_point(aes(x = upper), alpha = 0) +\n  #geom_point(aes(x = lower), alpha = 0) +\n  # Line going up from best case model\n  geom_segment(aes(x = true_value, xend = true_value), \n               y = \"Best case model\", yend = Inf, \n               color = \"grey30\", linetype = \"dotted\", size = 0.8) +\n  # Points for each run\n  geom_point(aes(fill = stage(model, after_scale = lighten(fill, 0.4))), \n             alpha = 0.7, size = 1.5, pch = 21, stroke = 0,\n             position = position_jitterdodge(jitter.width = 0.8, seed = 1)) +\n  # Error lines\n  stat_summary(geom = \"linerange\",\n               fun.min = function(z) {quantile(z, 0.025)},\n               fun.max = function(z) {quantile(z, 0.975)},\n               position = position_dodge(width = 0.75),\n               size = 1.3) +\n  # Point for mean\n  geom_point(aes(x = mean), size = 3) +\n  # Numeric text at mean\n  geom_text(aes(x = mean, \n                y = model, \n                label = format(round(mean, digits=2), nsmall = 2)), \n            vjust = -1.5, family = f1, size = 5, color = col_text) +\n  # Color for point and line\n  scale_color_manual(values = color_pal) +\n  scale_fill_manual(values = color_pal) +\n  # One plot for each variable\n  facet_wrap(vars(variable), \n             nrow = 1,\n             labeller = label_parsed, \n             scales = \"free_x\") +\n  # x-axis breaks\n  scale_x_continuous(breaks = seq(0, 4, 1)) +\n  # Lables\n  labs(x = \"Posterior mean\") +\n  # Add theme\n  theme_model_summary\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](simulation_study_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggsave(\"figures/simulation_boxplot.pdf\", \n       width = 10, height = 4, dpi = 600)\nggsave(\"figures/simulation_boxplot.eps\", width = 10, height = 4, dpi = 600, \n       device = cairo_ps)\n```\n:::\n",
    "supporting": [
      "simulation_study_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}