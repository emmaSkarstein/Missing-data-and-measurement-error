---
title: "Simulation study"
execute: 
  freeze: true
knitr:
  opts_chunk: 
    message: false
---

```{r}
#| purl: false
#| include: false
#knitr::purl(input = "simulation_study.qmd", output = "R/simulation_study.R")
```

```{r}
#| label: packages
#| message: false
library(INLA)
library(inlabru)
library(tidyverse)
```

```{r}
#| echo: false
inla.setOption(num.threads = "1:1")
```


In the vignette [Simulation example](Simulation_example.Rmd), we simulate a single data set with Berkson error, classical error and missing data, and then fit a measurement error model to adjust for these errors. In this simulation study, we do the exact same steps, but repeated on 100 simulated data sets instead of just one, to ensure that the results are not an artifact of one particular data set. This vignette consists of mostly just code, for detailed explanations on the steps taken in the analysis, please refer to [Simulation example](Simulation_example.qmd).


## Setting up functions

### Function for simulating data


```{r}
#| label: function for data simulation
simulate_data <- function(n){
  # Covariate without error:
  z <- rnorm(n, mean = 0, sd = 1)
  
  # Berkson error:
  u_b <- rnorm(n)
  r <- rnorm(n, mean = 1 + 2*z, sd = 1)
  x <- r + u_b
  
  # Response:
  y <- 1 + 2*x + 2*z + rnorm(n)
  
  # Classical error:
  u_c <- rnorm(n)
  w <- r + u_c # Use w_b here.
  
  # Missingness:
  m_pred <- -1.5 - 0.5*z # This gives a mean probability of missing of ca 0.2.
  m_prob <- exp(m_pred)/(1 + exp(m_pred))
  m_index <- rbinom(n, 1, prob = m_prob) # MAR
  # m_index <- sample(1:n, 0.2*n, replace = FALSE) # MCAR
  w[m_index] <- NA

  simulated_data <- data.frame(y = y, w = w, z = z, x = x)
  return(simulated_data)
}
```


### Functions for setting up the model matrices


```{r}
#| label: functions for model matrices
# Make matrix for ME model
make_matrix_ME <- function(data){
  n <- nrow(data)
  
  y <- data$y
  w <- data$w
  z <- data$z
  
  Y <- matrix(NA, 4*n, 4)

  Y[1:n, 1] <- y                 # Regression model of interest response
  Y[n+(1:n), 2] <- rep(0, n)     # Berkson error model response
  Y[2*n+(1:n), 3] <- w           # Classical error model response
  Y[3*n+(1:n), 4] <- rep(0, n)   # Imputation model response

  beta.0 <- c(rep(1, n), rep(NA, 3*n))
  beta.x <- c(1:n, rep(NA, 3*n))
  beta.z <- c(z, rep(NA, 3*n))

  id.x <- c(rep(NA, n), 1:n, rep(NA, n), rep(NA, n))
  weight.x <- c(rep(NA, n), rep(1, n), rep(NA, n), rep(NA, n))

  id.r <- c(rep(NA, n), 1:n, 1:n, 1:n)
  weight.r <- c(rep(NA, n), rep(-1, n), rep(1, n), rep(-1, n))

  alpha.0 = c(rep(NA, 3*n), rep(1, n))
  alpha.z = c(rep(NA, 3*n), z)
  
  dd_adj <- list(Y = Y,
                       beta.0 = beta.0,
                       beta.x = beta.x,
                       beta.z = beta.z,
                       id.x = id.x, 
                       weight.x = weight.x,
                       id.r = id.r,
                       weight.r = weight.r,
                       alpha.0 = alpha.0,
                       alpha.z = alpha.z)

  return(dd_adj)
}

# Make matrix for naive model
make_matrix_naive <- function(data){
  y <- data$y
  w <- data$w
  z <- data$z
  
  # Naive model
  dd_naive <- list(Y = y,
                         beta.0 = rep(1, nrow(data)),
                         beta.x = w, 
                         beta.z = z)
  return(dd_naive)
}

# Make matrix for model using the unobserved variable
make_matrix_true <- function(data){
  y <- data$y
  x <- data$x
  z <- data$z
  # True model
  dd_naive <- list(Y = y,
                         beta.0 = rep(1, nrow(data)),
                         beta.x = x, 
                         beta.z = z)
}
```



### Function for fitting the ME model

```{r}
#| label: function for fitting ME model
# Fit ME model
fit_model_ME <- function(data_matrix) {
  # Priors for model of interest coefficients
  prior.beta <- c(0, 1/1000) # N(0, 10^3)
  
  # Priors for exposure model coefficients
  prior.alpha <- c(0, 1/10000) # N(0, 10^4)
  
  # Priors for y, measurement error and true x-value precision
  prior.prec.y <- c(0.5, 0.5) # Gamma(0.5, 0.5)
  prior.prec.u_b <- c(0.5, 0.5) # Gamma(0.5, 0.5)
  prior.prec.u_c <- c(0.5, 0.5) # Gamma(0.5, 0.5)
  prior.prec.x <- c(0.5, 0.5) # Gamma(0.5, 0.5)
  
  # Initial values
  prec.y <- 1
  prec.u_b <- 1
  prec.u_c <- 1
  prec.x <- 1
  
  # Formula
  formula = Y ~ - 1 + beta.0 + beta.z +
    f(beta.x, copy = "id.x",  
      hyper = list(beta = list(param = prior.beta, fixed = FALSE))) +
    f(id.x, weight.x, model = "iid", values = 1:n, 
      hyper = list(prec = list(initial = -15, fixed = TRUE))) +
    f(id.r, weight.r, model="iid", values = 1:n, 
      hyper = list(prec = list(initial = -15, fixed = TRUE))) + 
    alpha.0 + alpha.z
  
  # Fit model
  model <- inla(formula,
                data = data_matrix,
                family = c("gaussian", "gaussian", "gaussian", "gaussian"),
                control.family = list(
                  list(hyper = list(prec = list(initial = log(prec.y), 
                                                param = prior.prec.y, 
                                                fixed = FALSE))), 
                  list(hyper = list(prec = list(initial = log(prec.u_b),
                                                  param = prior.prec.u_b,
                                                  fixed = FALSE))),
                  list(hyper = list(prec = list(initial = log(prec.u_c), 
                                                param = prior.prec.u_c, 
                                                fixed = FALSE))), 
                  list(hyper = list(prec = list(initial = log(prec.x), 
                                                param = prior.prec.x, 
                                                fixed = FALSE)))), 
                control.predictor = list(compute = TRUE), 
                control.fixed = list(
                  mean = list(
                    beta.0 = prior.beta[1],
                    beta.z = prior.beta[1],
                    alpha.0 = prior.alpha[1],
                    alpha.z = prior.alpha[1]),
                  prec = list(
                    beta.0 = prior.beta[2],
                    beta.z = prior.beta[2],
                    alpha.0 = prior.alpha[2],
                    alpha.z = prior.alpha[2])
    )
  )
}
```


### Function for fitting the true/naive model

The same function can be used to fit the naive model (`y ~ w + z`) and the best-case model (`y ~ x + z`) since they simply differ in the variable that is inputted (`w` versus `x`).


```{r}
#| label: function for fitting true/naive model
fit_model_naive_true <- function(data_matrix){
  # Priors for model of interest coefficients
  prior.beta <- c(0, 1/1000) # N(0, 10^3)

  # Priors for y, measurement error and true x-value precision
  prior.prec.y <- c(0.5, 0.5) # Gamma(0.5, 0.5)
  
  # Initial values
  prec.y <- 1

  # Formula
  formula <- Y ~ beta.0 - 1 + beta.x + beta.z
  
  # Fit model
  model <- inla(formula,
                data = data_matrix,
                family = c("gaussian"),
                control.family = list(
                  list(hyper = list(prec = list(initial = log(prec.y), 
                                                param = prior.prec.y, 
                                                fixed = FALSE)))),
                control.fixed = list(
                  mean = list(
                    beta.0 = prior.beta[1],
                    beta.z = prior.beta[1],
                    beta.x = prior.beta[1]),
                  prec = list(
                    beta.0 = prior.beta[2],
                    beta.z = prior.beta[2],
                    beta.x = prior.beta[2])
    )
  )
}
```




## Fitting the model for each data set

We simulate 100 data sets and fit the model that accounts for measurement error and missing data, and then save the posterior means for the intercept ans slopes.

Note that this chunk may take a while to run.


```{r}
#| label: running the simulation 
# Number of iterations
niter <- 100

# Data frames to store the results 
results_ME <- data.frame(matrix(NA, nrow=niter, ncol=5))
names(results_ME) <- c("beta.0", "beta.x", "beta.z", "alpha.0", "alpha.z")

results_naive <- data.frame(matrix(NA, nrow=niter, ncol=3))
names(results_naive) <- c("beta.0", "beta.x", "beta.z")

results_true <- data.frame(matrix(NA, nrow=niter, ncol=3))
names(results_true) <- c("beta.0", "beta.x", "beta.z")


for(i in 1:niter){
  n <- 1000
  data <- simulate_data(n)
  
  # ME model
  matrix_ME <- make_matrix_ME(data)
  model_ME <- fit_model_ME(matrix_ME)
  
  # Naive model
  matrix_naive <- make_matrix_naive(data)
  model_naive <- fit_model_naive_true(matrix_naive)
  
  # True model
  matrix_true <- make_matrix_true(data)
  model_true <- fit_model_naive_true(matrix_true)
  
  results_ME[i, c("beta.0", "beta.z", 
                  "alpha.0", "alpha.z")] <- t(model_ME$summary.fixed["mean"])
  results_ME[i, "beta.x"] <- model_ME$summary.hyperpar[3, "mean"]
  
  results_naive[i, c("beta.0", "beta.z", "beta.x")] <- t(model_naive$summary.fixed["mean"])
  
  results_true[i, c("beta.0", "beta.z", "beta.x")] <- t(model_true$summary.fixed["mean"])

}

```


## Results


```{r}
#| label: preparing results
#| echo: false
#| fig.showtext: true
library(ggplot2)
library(showtext)
library(ghibli)
library(colorspace)

## Boxplot
joint_results <- bind_rows(ME = results_ME, 
                           naive = results_naive, 
                           true = results_true, 
                           .id = "model") |> 
  pivot_longer(cols = 2:6, names_to = "variable") |> 
  filter(variable %in% c("beta.0", "beta.z", "beta.x")) |> 
  mutate(variable = tools::toTitleCase(gsub(pattern = ".*beta.", replacement = "", variable)))
```

```{r}
saveRDS(joint_results, file = "results/simulation_results.rds")
```

```{r}
#| label: visualizing results
#| echo: false
simulation_results <- readRDS("results/simulation_results.rds") %>% 
  mutate(variable = paste0("beta[", variable, "]")) %>% 
  mutate(model = fct_relevel(model, levels = c("naive", "ME", "true"))) %>% 
  mutate(model = recode(model, "ME" = "ME model", "true" = "True model", 
                        "naive" = "Complete case\nmodel"))
  
# Colors
col_bgr <- "white" #"#fbf9f4"
col_text <- "#191919"
color_pal <- c("#BB5566", "#DDAA33", "#004488")

# Loading fonts
f1 <- "Open Sans"
f2 <- "Open Sans"
font_add_google(name = f1, family = f1)
font_add_google(name = f2, family = f2)

showtext_auto()
showtext_opts(dpi = 300)

theme_titles <- theme_minimal(base_size = 18, base_family = f1) + 
  theme(
  axis.title.x = element_blank(),
  axis.title.y = element_text(size = 12, color = col_text, family = f1),
  axis.text = element_text(size = 10, color = col_text),
  axis.text.x = element_blank(),
  axis.ticks = element_blank(),
  legend.title = element_blank(),
  legend.text = element_text(size = 10),
  panel.background = element_rect(fill = col_bgr, color = col_bgr),
  plot.background = element_rect(fill = col_bgr, color = col_bgr, size = 1),
  legend.position = "none",
  strip.placement = "outside",
  strip.text = element_text(color = col_text),
  panel.grid.major.x = element_blank(),
  panel.grid.minor.x = element_blank(),
  plot.title.position = "plot",
  axis.text.y = element_text(family = f1),
  axis.line.x = element_line(size = 1, color = "grey65"),
  plot.margin = margin(rep(15, 4))
)

ggplot(simulation_results, aes(x = 1, y = value, groups = model)) +
  geom_point(aes(color = stage(model, after_scale = lighten(color, 0.4))), 
             alpha = 0.5, size = 0.5,
             position=position_jitterdodge(jitter.width = 0.2)) +
  # Error bar for confidence interval
  stat_summary(aes(color = model),
               geom = "linerange",
               fun.max = function(y) mean(y) + 1.96*sd(y),
               fun.min = function(y) mean(y) - 1.96*sd(y),
               position = position_dodge(width = 0.75),
               size = 0.8) +
  # Point at mean
  stat_summary(aes(color = model),
               fun = "mean", geom = "point",
               position = position_dodge(width = 0.75),
               size = 2) +
  # Text label
  stat_summary(data = dplyr::filter(simulation_results, variable == "beta[0]"),
               aes(x = 1, y = value + 0.15*value, label = model,
                   color = model),
               geom = "text", fun = max,
               position = position_dodge(width = 0.75),
               size = 3.3, fontface = "bold", family = f1) +
  # Y axis
  #scale_y_continuous(breaks = seq(1, 4.5, by = 0.5)) +
  # Facet by coefficient
  facet_wrap(~ variable, switch = "x",
             labeller = label_parsed) +
  # Don't remove points outside the plot
  coord_cartesian(clip = "off") +
  # Color of lines and points
  scale_color_manual(values = color_pal) +
  ylab("Posterior mean") +
  theme_titles

ggsave("figures/simulation_boxplot.png", 
       width = 10, height = 6.67)
```

